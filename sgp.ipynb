{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b43613e7",
   "metadata": {},
   "source": [
    "# GP Tutorial-E01: Simple GP and Its Implementation\n",
    "Despite that Gaussian process (GP) has been a long-existing and common used technique, which comes with many useful tutorials, books, articles, and blogs, I found that many (including me) struggled to put GP into real projects. I believe that the easiest way to learn something is by doing it, instead of preparing all the necessary mathematical backgrounds and diving into theory. Thus, I wrote this tutorials, trying to make it straight-forward to jump into GP. Hopes that you find it useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927ed342",
   "metadata": {},
   "source": [
    "In this tutorial, we consider a simple case of single output problem, i.e., $y \\in \\mathcal{R}^1$ is a scalar value, corresponding to $\\mathcal{x} \\in \\mathcal{R}^D$, a $D$-value input vector variable.\n",
    "<!-- For the tutorial purpose, we do not  -->\n",
    "<!-- 为了能够降低阅读的难度，我们在Tutorial-E01中将介绍最为简单的GP的代码，也就是指输出$y \\in \\mathcal{R}^1$为1维标量的情况。 -->\n",
    "<!-- 为了能够更加方便阅读以及理解，我们将GP整体的类定义拆分成了相互独立的函数.在最后会将全部包括定义成class的代码展示 -->\n",
    "\n",
    "Let's get started. This implementation is based on Torch, so let's start from importing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "50c77362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "\n",
    "print(torch.__version__)\n",
    "# I use torch (1.11.0) for this work.\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True' # mac os环境下有时候会报错。这个语句能解决这个问题。\n",
    "# from torch.autograd import Variable\n",
    "JITTER = 1e-6\n",
    "EPS = 1e-10\n",
    "PI = 3.1415"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8d3d8c",
   "metadata": {},
   "source": [
    "#### 1. Generating synthetic data\n",
    "<!-- 假设真实函数为$y = (6x-2)^2sin(12x-4)$。训练数据 $xtr, ytr$ 分别是$(32, 1)$的列向量，测试数据集$xte, yte$则是选择了100个点，是$(100, 1)$的列向量。 -->\n",
    "Consider the following function as our target function.\n",
    "$$y = (6x-2)^2sin(12x-4)$$\n",
    "We will generate training set $xtr, ytr$ containing 32 samples and testing set $xte, yte$ 100 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7692b6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xtr.size: torch.Size([32, 1]) ytr.size: torch.Size([32, 1])\n",
      "xte.size: torch.Size([100, 1]) yte.size: torch.Size([100, 1])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#train_set\n",
    "xtr = torch.rand(32, 1)\n",
    "ytr = ((6*xtr - 2)**2) * torch.sin(12*xtr - 4) + torch.randn(32, 1) * 1\n",
    "\n",
    "#test_set\n",
    "xte = torch.linspace(0, 1, 100).view(-1,1)\n",
    "yte = ((6*xte - 2)**2) * torch.sin(12*xte - 4)\n",
    "\n",
    "#plot the data\n",
    "print(\"xtr.size:\", xtr.size(), \"ytr.size:\", ytr.size())\n",
    "print(\"xte.size:\", xte.size(), \"yte.size:\", yte.size())\n",
    "plt.plot(xtr.numpy(), ytr.numpy(), 'b+')\n",
    "plt.plot(xte.numpy(), yte.numpy(), 'r-', alpha = 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32aeed6",
   "metadata": {},
   "source": [
    "<!-- #### 2. 设定可优化参数以及核函数 -->\n",
    "#### 2. Kernel functions and model parameter (hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a97c038",
   "metadata": {},
   "source": [
    "There are two functions, the mean function and the kernel function, that we need to optimize.\n",
    "We cannot optimize a \"function\" easily. Thus we give these functions a particular form to optimize their parameters.\n",
    "In this tutorial, we specify the mean function as a zero function (and there are nothing to optimize for it.)\n",
    "For the kernel function, we use the most commonly used automatic relevance determinant (ARD) kernel:\n",
    "$$k_{ard}(\\mathbf{x}, \\mathbf{x}') = a \\cdot \\exp(-\\frac{1}{2} \\sum_{d=1}^D{ (\\frac{x_d-x'_d}{l_d}})^2 )$$\n",
    "where $a$ is the amplitude of the kernel function, and $l_d$ is called length scale for d-dimensional input ${x}_d$. We can see that $\\mathbf{l}$ controls the contribution of each input dimension, e.g., a larger $l_d$ means that $x_d$ has a larger contribution (compared to other dimensions) on the system and vise versa. Thus this kernel has the name \"ARD\".\n",
    "\n",
    "One more thing before we start coding. Note that $l_d$ and $a$ are both positive parameters. If we directly define them as torch parameters, they might be pushed to negative during the optimization. Thus we need to make sure that they are positive. One simple way to ensure that is to define them as their logarithmic values and take their exponential values when we use them.\n",
    "\n",
    "Now let's define the kernel function, which returns the kernel matrix $K$ for two sets for input data.\n",
    "\n",
    "\n",
    "<!-- 高斯过程中，需要优化的变量是GP的均值函数（中的参数）以及核函数（中的参数）。\n",
    "在这个例子中我们假设均值函数为0，因此不需要优化他。\n",
    "<!-- 在这个最简单的情况下，我们已经设定均值函数为0，所以只需要优化协方差函数，也就是这个模型的kernel，因此在下面的cell里里设定的需要优化的参数其实就是kernel中的超参数。 -->\n",
    "<!-- 我们采用最为经典的 automatic relevance determinant （ARD）核函数： -->\n",
    "<!-- 这里$l_d$对输入的每个维度进行一个缩放，把那些对系统影响最大的因素（对应着$l_d$很大）筛选出来，所以叫做ARD；$a$ 是kernel的scale。 -->\n",
    "<!-- 计算kernel matrix的时候我们通常传入的是多个$\\mathbf{x}$, -->\n",
    "<!-- 另外，由于$l_d$ 和 $a$ 必须大于0。因此我们在写程序的时候定义的是 $\\log(l_d)$。当需要使用的时候再对其进行指数运算，这样保证我们的参数$\\exp{(\\log(l_d))}$永远是正的。 -->\n",
    "\n",
    "<!-- 核函数返回是两个或两组输入之间的核矩阵，我们可以简单地定义核函数： --> \n",
    "\n",
    "\n",
    "<!-- 下面的cell中我们定义了核函数，具体的核函数表达式为(SE/OU)：$$k_{ard} = a\\cdot exp(-\\frac{(x-x')^2-x^2-x'^2}{a})$$ 上述的公式仅是点对点的kernel的值，为了更好的处理大量的数据我们定义的kernel函数将返回一个大小为$(n_X,n_{X2})$的矩阵，其中的第i行，j列元素的含义是：$K_{ij} = k(X_i,X2_j)$。 -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2190321a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define kernel parameters\n",
    "log_length_scale = nn.Parameter(torch.zeros(xte.size(1)))\n",
    "log_scale = nn.Parameter(torch.zeros(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "019a10f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.8751, 0.8289,  ..., 0.9953, 0.9716, 0.9671],\n",
      "        [0.8751, 1.0000, 0.9954,  ..., 0.9159, 0.7511, 0.9673],\n",
      "        [0.8289, 0.9954, 1.0000,  ..., 0.8757, 0.6953, 0.9392],\n",
      "        ...,\n",
      "        [0.9953, 0.9159, 0.8757,  ..., 1.0000, 0.9447, 0.9871],\n",
      "        [0.9716, 0.7511, 0.6953,  ..., 0.9447, 1.0000, 0.8832],\n",
      "        [0.9671, 0.9673, 0.9392,  ..., 0.9871, 0.8832, 1.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def kernel(X1, X2, log_length_scale, log_scale): \n",
    "    length_scale = torch.exp(log_length_scale)\n",
    "    K = torch.zeros(X1.size(0), X2.size(0))\n",
    "    \n",
    "    for i in range(X1.size(0)):\n",
    "        for j in range(X2.size(0)):\n",
    "            for d in range(X1.size(1)):\n",
    "                K[i,j] = torch.exp(-0.5 * ((X1[i,d] - X2[j,d])**2 / length_scale[d]**2).sum() ) * torch.exp(log_scale)\n",
    "    return log_scale.exp() * K\n",
    "\n",
    "K1 = kernel(xtr, xtr, log_length_scale, log_scale)\n",
    "print(K1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fb9268",
   "metadata": {},
   "source": [
    "*However, we should *NEVER** use a for loop unless we have to. \n",
    "Using the for loop will take a lot time and memory we should have not paid.\n",
    "Let's redo the kernel function in a different way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c88c4cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.8751, 0.8289,  ..., 0.9953, 0.9716, 0.9671],\n",
      "        [0.8751, 1.0000, 0.9954,  ..., 0.9159, 0.7511, 0.9673],\n",
      "        [0.8289, 0.9954, 1.0000,  ..., 0.8757, 0.6953, 0.9392],\n",
      "        ...,\n",
      "        [0.9953, 0.9159, 0.8757,  ..., 1.0000, 0.9447, 0.9871],\n",
      "        [0.9716, 0.7511, 0.6953,  ..., 0.9447, 1.0000, 0.8832],\n",
      "        [0.9671, 0.9673, 0.9392,  ..., 0.9871, 0.8832, 1.0000]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "tensor(6.9765e-07, grad_fn=<CopyBackwards>)\n"
     ]
    }
   ],
   "source": [
    "def kernel(X1, X2, log_length_scale, log_scale): # 定义核函数没有加linear\n",
    "\n",
    "    X1 = X1 / log_length_scale.exp()**2\n",
    "    X2 = X2 / log_length_scale.exp()**2\n",
    "\n",
    "    X1_norm2 = X1 * X1\n",
    "    X2_norm2 = X2 * X2\n",
    "\n",
    "    K = -2.0 * X1 @ X2.t() + X1_norm2.expand(X1.size(0), X2.size(0)) + X2_norm2.t().expand(X1.size(0), X2.size(0))  #this is the effective Euclidean distance matrix between X1 and X2.\n",
    "    K = log_scale.exp() * torch.exp(-0.5 * K)\n",
    "    return K\n",
    "\n",
    "K2 = kernel(xtr, xtr, log_length_scale, log_scale)\n",
    "print(K2)\n",
    "print( (K1 - K2).norm() )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fa4bed",
   "metadata": {},
   "source": [
    "We can see that the resulting kernel is the same, whereas the execution time is much faster. You can test a large xtr to see the difference.\n",
    "Wo also need to define the noise $\\sigma^2$ of the GP.\n",
    "To be easier to introduce conjugate prior in a later tutorial, instead of defining $\\sigma^2$, we define its inverse $\\beta=1/\\sigma^2$. Again, we need the value positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "34dba8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_beta = nn.Parameter(torch.ones(1) * -4) # this is a large noise. we optimize to shrink it to a proper value. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932c072b",
   "metadata": {},
   "source": [
    "<!-- #### 3. 定义loss损失函数以及优化器 -->\n",
    "#### 3. Negative log likelihood (nll) \n",
    "For almost all regression problems, we aim to minimize a defined loss function. In the GP case, the loss function is the negative log likelihood.\n",
    "It is negative because we want to minimize as like what we do for a loss function; we take the logarithm of the likelihood because it gives a clear form, and the logarithm does not change the monotonicity of the function.\n",
    "The log likelihood is defined as:\n",
    "$$L=-\\frac{1}{2}\\mathbf{y}^T (\\mathbf{K}+\\sigma^2 \\mathbf{I})^{-1}\\mathbf{y}-\\frac{1}{2}\\log(|\\mathbf{K}+\\sigma^2 \\mathbf{I}|)-\\frac{n}{2}log(2\\pi)$$\n",
    "where $\\mathbf{y}$ is the observed data arranged as a $N\\times1$ vector, $\\mathbf{K}$ is the $N\\times N$ kernel matrix (of the training data), $\\sigma^2$ is the noise variance, and $\\mathbf{I}$ is the identity matrix.\n",
    "Rewrite our loss function in nll, we have\n",
    "$$nll=\\frac{1}{2}\\mathbf{y}^T\\mathbf{\\Sigma}^{-1}\\mathbf{y}+\\frac{1}{2}\\log(|\\mathbf{\\Sigma}|)+\\frac{n}{2}log(2\\pi)$$\n",
    "where we define $\\mathbf{\\Sigma}=\\mathbf{K}+\\sigma^2 \\mathbf{I}$ as the inverse of the kernel matrix.\n",
    "\n",
    "We can directly solve code up the nll function and call it in the optimizer. However, we should NOT inverse a matrix unless we have to.\n",
    "To calculate the nll in a more efficeint way and stable, we can use the following trick:\n",
    "\n",
    "First compute the cholesky decomposition of the kernel matrix, $\\mathbf{\\Sigma} = \\mathbf{L} \\mathbf{L}^T$.\n",
    "For the first term in the nll, we have,\n",
    "$$ \\mathbf{y}^T (\\mathbf{K}+\\sigma^2 \\mathbf{I})^{-1}\\mathbf{y} = \\mathbf{y}^T (\\mathbf{L} \\mathbf{L}^T)^{-1} \\mathbf{y} = \\mathbf{y}^T \\mathbf{L}^{-T} \\mathbf{L}^{-1} \\mathbf{y}  = (\\mathbf{L}^{-1} \\mathbf{y})^T \\mathbf{L}^{-1} \\mathbf{y}  $$\n",
    "The formula allows us to avoid inverting the kernel matrix by solving a linear system, $\\mathbf{L} \\mathbf{\\gamma} = \\mathbf{y}$, which gives us $\\mathbf{L}^{-1} \\mathbf{y} = \\mathbf{\\gamma}$.\n",
    "Once we obtain $\\mathbf{\\gamma}$, we can calculate the first term in nll as the L2 norm of  $\\mathbf{\\gamma}$.\n",
    "\n",
    "<!-- Remember that we should not invert a matrix unless we have to.  -->\n",
    "<!-- Define , which will becomes handy lately. -->\n",
    "For the second term in the nll, we have,\n",
    "$$ \\log(|\\mathbf{\\Sigma}|) = \\log(| \\mathbf{L} \\mathbf{L}^T |) =  \\log(|\\mathbf{L}| |\\mathbf{L}^T|) = \\log(\\prod_{i=1}^{N} L_{ii} \\prod_{i=1}^{N} L_{ii}) = 2\\sum_{i=1}^{N}\\log(L_{ii})$$\n",
    "\n",
    "With these two process, the nll becomes:\n",
    "$$nll=\\frac{1}{2} \\mathbf{\\gamma}^T \\mathbf{\\gamma} + \\sum_{i=1}^{N}\\log(L_{ii}) + \\frac{n}{2}log(2\\pi)$$\n",
    "\n",
    "Let's code these up now. It will turn out simple.\n",
    "\n",
    "<!-- 在高斯过程中，我们是通过找极大似然函数对GP进行优化，那么在这里我们的loss就应该是这个模型的似然函数（likelihood）。为了能够更方便计算，通常会对似然函数取log，且为了方便torch中的优化器调用，可以利用求极小值的优化器来对log_likelihood的相反数进行优化。因此下面cell中的negative_log_likelihood函数是模型的-log(likelihood)。\n",
    "$$-L=\\frac{1}{2}y^T\\Sigma^{-1}y+\\frac{1}{2}log(|\\Sigma|)+\\frac{n}{2}log(2\\pi)$$\n",
    "其中$\\Sigma = K(X,X)+ \\frac{1}{\\beta} I+ Jitter\\cdot I$，是加上噪声影响后的协方差矩阵。\n",
    "\n",
    "但是，由于对一个矩阵求逆计算复杂且耗时较长，因此我们采用cholesky分解，将协方差矩阵$\\Sigma$分解为下三角矩阵$L$和其转置矩阵的乘积，也就是：$\\Sigma = LL^T$，因此我们可以先计算中间变量$\\alpha = L^{-1}Y$，降低计算的复杂度，具体推导如下：\n",
    "\\begin{aligned}\n",
    "-L=&\\frac{1}{2}y^T\\Sigma^{-1}y+\\frac{1}{2}log(|\\Sigma|)+\\frac{n}{2}log(2\\pi)\\\\\n",
    "=&\\frac{1}{2}y^T(LL^T)^{-1}y+\\frac{1}{2}log(|LL^T|)+\\frac{n}{2}log(2\\pi)\\\\\n",
    "=&\\frac{1}{2}(yL^{-1})^TyL^{-1}+\\frac{1}{2}log(|LL^T|)+\\frac{n}{2}log(2\\pi)\\\\\n",
    "=&\\alpha^T\\alpha+\\frac{1}{2}log(|\\prod_{i=1}^nL_{ii}|)+\\frac{n}{2}log(2\\pi)\\\\\n",
    "=&\\sum_{i=1}^{n}\\alpha_i^2+\\sum_{i=1}^{n}L_{ii}+\\frac{n}{2}log(2\\pi)\n",
    "\\end{aligned}\n",
    "在计算$\\Sigma$矩阵行列式时利用cholesky分解的特点，有结论：$|\\Sigma|=|\\prod_{i=1}^nL_{ii}|$，返回值为标量。 -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f3b4cd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_log_likelihood(X, Y, log_length_scale, log_scale, log_beta):\n",
    "    y_num = Y.size(0)\n",
    "    Sigma = kernel(X, X, log_length_scale, log_scale) + log_beta.exp().pow(-1) * torch.eye(X.size(0)) + JITTER * torch.eye(X.size(0))   # add JITTER here to avoid singularity\n",
    "    \n",
    "    L = torch.linalg.cholesky(Sigma)\n",
    "    #option 1 (use this if torch supports)\n",
    "    gamma,_ = torch.triangular_solve(Y, L, upper = False)\n",
    "    #option 2\n",
    "    # gamma = L.inverse() @ Y       # we can use this as an alternative because L is a lower triangular matrix.\n",
    "    \n",
    "    nll =  0.5 * (gamma ** 2).sum() +  L.diag().log().sum()  + 0.5 * y_num * torch.log(2 * torch.tensor(PI))\n",
    "    return nll"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0218da11",
   "metadata": {},
   "source": [
    "<!-- 当设定好loss函数以后就可以借用torch.optim中的优化器对模型进行优化，求极小值。 -->\n",
    "#### 4. Calling the optimizer\n",
    "With the loss/nll defined, we can simply call an optimizer to minimize the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "28a06a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0  nnl: 61.25714111328125\n",
      "0.3011\n",
      "iter 1  nnl: 61.244903564453125\n",
      "0.3077\n",
      "iter 2  nnl: 61.25049591064453\n",
      "0.3080\n",
      "iter 3  nnl: 61.25164031982422\n",
      "0.3051\n",
      "iter 4  nnl: 61.247955322265625\n",
      "0.3010\n",
      "iter 5  nnl: 61.24506378173828\n",
      "0.2973\n",
      "iter 6  nnl: 61.24583435058594\n",
      "0.2952\n",
      "iter 7  nnl: 61.24802780151367\n",
      "0.2951\n",
      "iter 8  nnl: 61.24851989746094\n",
      "0.2966\n",
      "iter 9  nnl: 61.246910095214844\n",
      "0.2991\n",
      "iter 10  nnl: 61.24555206298828\n",
      "0.3018\n",
      "iter 11  nnl: 61.2449836730957\n",
      "0.3040\n",
      "iter 12  nnl: 61.24559783935547\n",
      "0.3051\n",
      "iter 13  nnl: 61.24693298339844\n",
      "0.3050\n",
      "iter 14  nnl: 61.24640655517578\n",
      "0.3039\n",
      "iter 15  nnl: 61.246055603027344\n",
      "0.3021\n",
      "iter 16  nnl: 61.24492645263672\n",
      "0.3001\n",
      "iter 17  nnl: 61.245262145996094\n",
      "0.2986\n",
      "iter 18  nnl: 61.245758056640625\n",
      "0.2977\n",
      "iter 19  nnl: 61.2459716796875\n",
      "0.2977\n",
      "iter 20  nnl: 61.24555969238281\n",
      "0.2986\n",
      "iter 21  nnl: 61.244850158691406\n",
      "0.2999\n",
      "iter 22  nnl: 61.245201110839844\n",
      "0.3013\n",
      "iter 23  nnl: 61.24494934082031\n",
      "0.3025\n",
      "iter 24  nnl: 61.245758056640625\n",
      "0.3031\n",
      "iter 25  nnl: 61.24555206298828\n",
      "0.3030\n",
      "iter 26  nnl: 61.24524688720703\n",
      "0.3024\n",
      "iter 27  nnl: 61.244873046875\n",
      "0.3014\n",
      "iter 28  nnl: 61.245208740234375\n",
      "0.3004\n",
      "iter 29  nnl: 61.245567321777344\n",
      "0.2997\n",
      "iter 30  nnl: 61.24518585205078\n",
      "0.2993\n",
      "iter 31  nnl: 61.24567413330078\n",
      "0.2995\n",
      "iter 32  nnl: 61.244728088378906\n",
      "0.3000\n",
      "iter 33  nnl: 61.245033264160156\n",
      "0.3008\n",
      "iter 34  nnl: 61.245140075683594\n",
      "0.3014\n",
      "iter 35  nnl: 61.24516296386719\n",
      "0.3018\n",
      "iter 36  nnl: 61.24488830566406\n",
      "0.3019\n",
      "iter 37  nnl: 61.24562072753906\n",
      "0.3016\n",
      "iter 38  nnl: 61.24528884887695\n",
      "0.3011\n",
      "iter 39  nnl: 61.245452880859375\n",
      "0.3005\n",
      "iter 40  nnl: 61.24519348144531\n",
      "0.3002\n",
      "iter 41  nnl: 61.24490737915039\n",
      "0.3002\n",
      "iter 42  nnl: 61.244876861572266\n",
      "0.3004\n",
      "iter 43  nnl: 61.244998931884766\n",
      "0.3008\n",
      "iter 44  nnl: 61.24474334716797\n",
      "0.3012\n",
      "iter 45  nnl: 61.245079040527344\n",
      "0.3015\n",
      "iter 46  nnl: 61.24467468261719\n",
      "0.3015\n",
      "iter 47  nnl: 61.24519348144531\n",
      "0.3013\n",
      "iter 48  nnl: 61.245262145996094\n",
      "0.3010\n",
      "iter 49  nnl: 61.244773864746094\n",
      "0.3006\n"
     ]
    }
   ],
   "source": [
    "def train_adam(X, Y, log_length_scale, log_scale, log_beta, niter = 10, lr = 0.001):\n",
    "    optimizer = torch.optim.Adam([log_beta, log_length_scale, log_scale], lr = lr)\n",
    "    optimizer.zero_grad()\n",
    "    for i in range(niter):\n",
    "        optimizer.zero_grad()\n",
    "        # self.update()\n",
    "        loss = negative_log_likelihood(X, Y, log_length_scale, log_scale, log_beta)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print the nll\n",
    "        print('iter', i, ' nnl:', loss.item())\n",
    "        print('nnl {:.4f}'.format(log_beta.item()) )\n",
    "        # print the likelihood\n",
    "        # print('iter', i, ' likelihood:', (-loss).exp().item())\n",
    "        \n",
    "train_adam(xtr, ytr,log_length_scale, log_scale, log_beta, 50, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd16b4d",
   "metadata": {},
   "source": [
    "<!-- #### 4. 预测 Predictive Prosterior  -->\n",
    "#### 5. Predictive posterior \n",
    "\n",
    "With the hyperparameters optimized, we can now calculate the predictive posterior following the standard formulations,\n",
    "\\begin{aligned}\n",
    "&\\mu=k^* \\Sigma^{-1} \\mathbf{y}\\\\\n",
    "&s^2=k_{**}- (k^*)^T \\mathbf{\\Sigma}^{-1} k^* + {1}/{\\beta}\\\\\n",
    "\\end{aligned}\n",
    "As we mentioned, we should not invert a matrix unless we have to.\n",
    "Again, we utilize the cholesky decomposition to obtain the predictive posterior.\n",
    "Let define $\\mathbf{\\Sigma}^{-1} \\mathbf{y} =  \\mathbf{\\alpha}$, which is conventionally used in the literature and open source codes. \n",
    "The advantage of introducing $\\mathbf{\\alpha}$ is that 1) we can use the cholesky decomposition to compute the predictive posterior; 2) it saves storage and provides quick prediction of the posterior (no more inversion required).\n",
    "<!-- Using $\\mathbf{\\gamma}$,  -->\n",
    "Using $\\mathbf{L}$ to get $\\mathbf{\\alpha}$, we solve the $\\mathbf{L} \\mathbf{\\gamma} = \\mathbf{y}$ first and then compute $\\mathbf{L}^T \\mathbf{y} = \\mathbf{\\gamma}$, which, as you might have seen, is sometimes written in a compact way\n",
    "$$\\mathbf{\\alpha} = \\mathbf{L}^T \\backslash \\mathbf{L} \\backslash \\mathbf{y}$$\n",
    "\n",
    "The computation of $(k^*)^T \\mathbf{\\Sigma}^{-1} k^*$ is similar to the computation of $\\mathbf{\\gamma}^T \\mathbf{\\gamma}$ in the previous section.\n",
    "\n",
    "<!-- 在预测模块，需要预测的主要为预测点的均值与方差，值得注意的是虽然模型假设均值函数为0但是预测点的均值不必须为0由预测值来决定。预测点的均值与方差表达式如下：\n",
    "\\begin{aligned}\n",
    "&\\mu=k(x^*,x)\\Sigma^{-1}Y=k(x^*,x)(LL^T)^{-1}Y\\\\\n",
    "&s^2=k_{**}-k(x,x^*)\\Sigma^{-1}k(x^*,x)\n",
    "\\end{aligned}\n",
    "为了方便计算，我们依旧利用cholesky分解，$\\Sigma=LL^T$，并且在方差部分由于函数设置是可以多个预测点同时进行计算，所以方差部分，我们仅需要预测方差矩阵的对角线部分的数值。因为噪声较小，所以课假设噪声不影响求逆。$$diag(s^2)=a\\cdot I-diag(k(x,x^*)(LL^T)^{-1}k(x^*,x))+\\frac{1}{\\beta}\\cdot I$$ -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c2a7a68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, Xte, log_length_scale, log_scale, log_beta, Y):\n",
    "    n_test = Xte.size(0)\n",
    "    Sigma = kernel(X, X, log_length_scale, log_scale) + log_beta.exp().pow(-1) * torch.eye(\n",
    "        X.size(0)) + JITTER * torch.eye(X.size(0))\n",
    "    kx = kernel(X, Xte, log_length_scale, log_scale)\n",
    "    L = torch.cholesky(Sigma)\n",
    "    \n",
    "    # option 1\n",
    "    mean = kx.t() @ torch.cholesky_solve(Y, L)  # torch.linalg.cholesky()\n",
    "    # option 2\n",
    "    # mean = kx @ torch.L.t().inverse() @ L.inverse() @ Y\n",
    "    \n",
    "    # LinvKx = L.inverse() @ kx.t()  # TODO: the inverse for L should be cheap. check this.\n",
    "        # torch.cholesky_solve(kx.t(), L)\n",
    "    LinvKx,_ = torch.triangular_solve(kx, L, upper = False)\n",
    "    # option 1, standard way\n",
    "    # var_diag = log_scale.exp().expand(n_test, 1) - (LinvKx.t() @ LinvKx).diag().view(-1, 1)\n",
    "    # option 2, a faster way\n",
    "    var_diag = log_scale.exp().expand(n_test, 1) - (LinvKx**2).sum(dim = 0).view(-1, 1)\n",
    "    \n",
    "    var_diag = var_diag + log_beta.exp().pow(-1)\n",
    "    return mean, var_diag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552b00a1",
   "metadata": {},
   "source": [
    "#### 6. Testing and validating\n",
    "let try our GP model on the synthetic data and see how well it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "5c1078f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0  nnl: 93.8595962524414\n",
      "iter 1  nnl: 93.06547546386719\n",
      "iter 2  nnl: 92.33882904052734\n",
      "iter 3  nnl: 91.68415069580078\n",
      "iter 4  nnl: 91.10782623291016\n",
      "iter 5  nnl: 90.61164855957031\n",
      "iter 6  nnl: 90.19174194335938\n",
      "iter 7  nnl: 89.84723663330078\n",
      "iter 8  nnl: 89.57742309570312\n",
      "iter 9  nnl: 89.38447570800781\n",
      "iter 10  nnl: 89.26968383789062\n",
      "iter 11  nnl: 89.2069320678711\n",
      "iter 12  nnl: 89.13294982910156\n",
      "iter 13  nnl: 88.9898681640625\n",
      "iter 14  nnl: 88.75384521484375\n",
      "iter 15  nnl: 88.43218994140625\n",
      "iter 16  nnl: 88.0539779663086\n",
      "iter 17  nnl: 87.65829467773438\n",
      "iter 18  nnl: 87.27020263671875\n",
      "iter 19  nnl: 86.86763763427734\n",
      "iter 20  nnl: 86.39346313476562\n",
      "iter 21  nnl: 85.81949615478516\n",
      "iter 22  nnl: 85.16242980957031\n",
      "iter 23  nnl: 84.45563507080078\n",
      "iter 24  nnl: 83.71517944335938\n",
      "iter 25  nnl: 82.91446685791016\n",
      "iter 26  nnl: 82.0051498413086\n",
      "iter 27  nnl: 80.96735382080078\n",
      "iter 28  nnl: 79.82384490966797\n",
      "iter 29  nnl: 78.62024688720703\n",
      "iter 30  nnl: 77.39315795898438\n",
      "iter 31  nnl: 76.1421127319336\n",
      "iter 32  nnl: 74.85182189941406\n",
      "iter 33  nnl: 73.5505599975586\n",
      "iter 34  nnl: 72.29100036621094\n",
      "iter 35  nnl: 71.08252716064453\n",
      "iter 36  nnl: 69.89517974853516\n",
      "iter 37  nnl: 68.72677612304688\n",
      "iter 38  nnl: 67.62240600585938\n",
      "iter 39  nnl: 66.61546325683594\n",
      "iter 40  nnl: 65.65818786621094\n",
      "iter 41  nnl: 64.75384521484375\n",
      "iter 42  nnl: 63.973876953125\n",
      "iter 43  nnl: 63.28620147705078\n",
      "iter 44  nnl: 62.67555236816406\n",
      "iter 45  nnl: 62.21143341064453\n",
      "iter 46  nnl: 61.84423828125\n",
      "iter 47  nnl: 61.57836151123047\n",
      "iter 48  nnl: 61.45021057128906\n",
      "iter 49  nnl: 61.387386322021484\n",
      "iter 50  nnl: 61.43049621582031\n",
      "iter 51  nnl: 61.51433563232422\n",
      "iter 52  nnl: 61.632232666015625\n",
      "iter 53  nnl: 61.7652587890625\n",
      "iter 54  nnl: 61.86658477783203\n",
      "iter 55  nnl: 61.95301818847656\n",
      "iter 56  nnl: 61.97676086425781\n",
      "iter 57  nnl: 61.97642517089844\n",
      "iter 58  nnl: 61.92266845703125\n",
      "iter 59  nnl: 61.84928894042969\n",
      "iter 60  nnl: 61.751739501953125\n",
      "iter 61  nnl: 61.650779724121094\n",
      "iter 62  nnl: 61.55506896972656\n",
      "iter 63  nnl: 61.464874267578125\n",
      "iter 64  nnl: 61.39630126953125\n",
      "iter 65  nnl: 61.34296417236328\n",
      "iter 66  nnl: 61.30823516845703\n",
      "iter 67  nnl: 61.292320251464844\n",
      "iter 68  nnl: 61.28753662109375\n",
      "iter 69  nnl: 61.293861389160156\n",
      "iter 70  nnl: 61.30841064453125\n",
      "iter 71  nnl: 61.325138092041016\n",
      "iter 72  nnl: 61.341468811035156\n",
      "iter 73  nnl: 61.358802795410156\n",
      "iter 74  nnl: 61.371063232421875\n",
      "iter 75  nnl: 61.37828826904297\n",
      "iter 76  nnl: 61.380950927734375\n",
      "iter 77  nnl: 61.37914276123047\n",
      "iter 78  nnl: 61.37211608886719\n",
      "iter 79  nnl: 61.361572265625\n",
      "iter 80  nnl: 61.349525451660156\n",
      "iter 81  nnl: 61.33483123779297\n",
      "iter 82  nnl: 61.32011413574219\n",
      "iter 83  nnl: 61.305904388427734\n",
      "iter 84  nnl: 61.29335021972656\n",
      "iter 85  nnl: 61.282569885253906\n",
      "iter 86  nnl: 61.274452209472656\n",
      "iter 87  nnl: 61.268043518066406\n",
      "iter 88  nnl: 61.26433563232422\n",
      "iter 89  nnl: 61.26190185546875\n",
      "iter 90  nnl: 61.26106262207031\n",
      "iter 91  nnl: 61.26286315917969\n",
      "iter 92  nnl: 61.26349639892578\n",
      "iter 93  nnl: 61.26409149169922\n",
      "iter 94  nnl: 61.26402282714844\n",
      "iter 95  nnl: 61.26457977294922\n",
      "iter 96  nnl: 61.263755798339844\n",
      "iter 97  nnl: 61.26263427734375\n",
      "iter 98  nnl: 61.25984191894531\n",
      "iter 99  nnl: 61.25768280029297\n",
      "iter 100  nnl: 61.2548828125\n",
      "iter 101  nnl: 61.25263214111328\n",
      "iter 102  nnl: 61.250099182128906\n",
      "iter 103  nnl: 61.24873352050781\n",
      "iter 104  nnl: 61.24660110473633\n",
      "iter 105  nnl: 61.2464599609375\n",
      "iter 106  nnl: 61.245361328125\n",
      "iter 107  nnl: 61.24518585205078\n",
      "iter 108  nnl: 61.244903564453125\n",
      "iter 109  nnl: 61.24554443359375\n",
      "iter 110  nnl: 61.245506286621094\n",
      "iter 111  nnl: 61.24657440185547\n",
      "iter 112  nnl: 61.24656677246094\n",
      "iter 113  nnl: 61.24706268310547\n",
      "iter 114  nnl: 61.24671936035156\n",
      "iter 115  nnl: 61.247032165527344\n",
      "iter 116  nnl: 61.247093200683594\n",
      "iter 117  nnl: 61.24689483642578\n",
      "iter 118  nnl: 61.2459716796875\n",
      "iter 119  nnl: 61.246009826660156\n",
      "iter 120  nnl: 61.246070861816406\n",
      "iter 121  nnl: 61.24578857421875\n",
      "iter 122  nnl: 61.24529266357422\n",
      "iter 123  nnl: 61.24531555175781\n",
      "iter 124  nnl: 61.24560546875\n",
      "iter 125  nnl: 61.24521255493164\n",
      "iter 126  nnl: 61.24530029296875\n",
      "iter 127  nnl: 61.2452392578125\n",
      "iter 128  nnl: 61.24552917480469\n",
      "iter 129  nnl: 61.245452880859375\n",
      "iter 130  nnl: 61.24555587768555\n",
      "iter 131  nnl: 61.245689392089844\n",
      "iter 132  nnl: 61.24537658691406\n",
      "iter 133  nnl: 61.24555206298828\n",
      "iter 134  nnl: 61.24531555175781\n",
      "iter 135  nnl: 61.24567413330078\n",
      "iter 136  nnl: 61.24524688720703\n",
      "iter 137  nnl: 61.245384216308594\n",
      "iter 138  nnl: 61.245025634765625\n",
      "iter 139  nnl: 61.24474334716797\n",
      "iter 140  nnl: 61.2451171875\n",
      "iter 141  nnl: 61.24493408203125\n",
      "iter 142  nnl: 61.244911193847656\n",
      "iter 143  nnl: 61.244834899902344\n",
      "iter 144  nnl: 61.24528503417969\n",
      "iter 145  nnl: 61.245025634765625\n",
      "iter 146  nnl: 61.245262145996094\n",
      "iter 147  nnl: 61.24457550048828\n",
      "iter 148  nnl: 61.245033264160156\n",
      "iter 149  nnl: 61.24518585205078\n",
      "iter 150  nnl: 61.24491882324219\n",
      "iter 151  nnl: 61.24525451660156\n",
      "iter 152  nnl: 61.24522399902344\n",
      "iter 153  nnl: 61.245079040527344\n",
      "iter 154  nnl: 61.24481964111328\n",
      "iter 155  nnl: 61.244873046875\n",
      "iter 156  nnl: 61.24540710449219\n",
      "iter 157  nnl: 61.24456787109375\n",
      "iter 158  nnl: 61.245460510253906\n",
      "iter 159  nnl: 61.24506378173828\n",
      "iter 160  nnl: 61.24504089355469\n",
      "iter 161  nnl: 61.24509811401367\n",
      "iter 162  nnl: 61.24479675292969\n",
      "iter 163  nnl: 61.24480056762695\n",
      "iter 164  nnl: 61.24481964111328\n",
      "iter 165  nnl: 61.244544982910156\n",
      "iter 166  nnl: 61.24555206298828\n",
      "iter 167  nnl: 61.24497985839844\n",
      "iter 168  nnl: 61.245018005371094\n",
      "iter 169  nnl: 61.245323181152344\n",
      "iter 170  nnl: 61.2451286315918\n",
      "iter 171  nnl: 61.24537658691406\n",
      "iter 172  nnl: 61.244869232177734\n",
      "iter 173  nnl: 61.244972229003906\n",
      "iter 174  nnl: 61.24488067626953\n",
      "iter 175  nnl: 61.244693756103516\n",
      "iter 176  nnl: 61.24480438232422\n",
      "iter 177  nnl: 61.24530029296875\n",
      "iter 178  nnl: 61.24439239501953\n",
      "iter 179  nnl: 61.24494934082031\n",
      "iter 180  nnl: 61.24523162841797\n",
      "iter 181  nnl: 61.24500274658203\n",
      "iter 182  nnl: 61.24513244628906\n",
      "iter 183  nnl: 61.24530029296875\n",
      "iter 184  nnl: 61.24492645263672\n",
      "iter 185  nnl: 61.245140075683594\n",
      "iter 186  nnl: 61.245147705078125\n",
      "iter 187  nnl: 61.24482727050781\n",
      "iter 188  nnl: 61.244873046875\n",
      "iter 189  nnl: 61.2451171875\n",
      "iter 190  nnl: 61.244873046875\n",
      "iter 191  nnl: 61.24482727050781\n",
      "iter 192  nnl: 61.24516296386719\n",
      "iter 193  nnl: 61.24512481689453\n",
      "iter 194  nnl: 61.24525451660156\n",
      "iter 195  nnl: 61.245452880859375\n",
      "iter 196  nnl: 61.24494171142578\n",
      "iter 197  nnl: 61.245094299316406\n",
      "iter 198  nnl: 61.24519348144531\n",
      "iter 199  nnl: 61.2442626953125\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_adam(xtr, ytr, log_length_scale, log_scale, log_beta, 200, 0.1)\n",
    "with torch.no_grad():\n",
    "    ypred, yvar = forward(xtr, xte, log_length_scale, log_scale, log_beta, ytr)\n",
    "    \n",
    "plt.errorbar(xte.numpy().reshape(100), ypred.detach().numpy().reshape(100),\n",
    "             yerr=yvar.sqrt().squeeze().detach().numpy(), fmt='r-.', alpha=0.2)\n",
    "plt.plot(xtr.numpy(), ytr.numpy(), 'b+')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
